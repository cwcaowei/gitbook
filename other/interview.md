#### 基于ShardingSphere将消息记录表分表及读写分离

- 读写分离

  长连接服务主要提供了消息推送和查询的功能，接入了7个业务线，日均消息量500万左右，一开始只有一张消息记录表，所有业务线的消息都存储在这张表中，后来随着业务的发展，接入的业务线越来越多，每天的消息量越来越多，我们发现有一个业务线只用了长连接，不需要查询，且数据量较大，占用总数据量的70%，所以把它和其他业务线的存储做了分离，代码层面它和其他业务线用的还是同一个逻辑表都叫MESSAGE_RECORD，实际的真实表命名上我们加了后缀作区分，为了保证插入的效率，这张表上面没有加任何索引，而其他业务线存储的表会有一些查询的索引，然后写了一个分片算法，根据业务线判断存到哪张真实表。

- 分表

  后来随着这些业务线的扩张，使用的人员越来越多，每天的消息量也越来越大，只用长连接的那个业务线一天就是300多万的消息，其他业务线加起来也有200万，所以我们开始考虑按月分表，我们改写了之前的分片算法，在基于业务线判断后又基于当前时间将数据存到对应的月份表中。

- 读

  查询的场景主要是根据用户ID分页查他未读、已读和全部的消息，并按时间倒序，如何解决多表的查询、分页和排序问题，之前入库的时候是以业务线和创建时间为分片键的一个分片算法，返回的是写入哪一个数据源，而查询的时候则是以用户ID为分片键的一个分片算法，会返回多个数据源，sharding jdbc会做sql的改写，从多个数据源查到数据后在内存中再做组合归并，最终返回。

#### 解决websocket服务频繁FULL GC的问题

生产上频繁出现Full GC，重启后短时间没问题，超过一定时间后，又开始频繁Full GC，通过mat分析dump文件，发现有一个叫ScheduledThreadPoolExecutor的类下面有个叫DelayedWorkQueue的类下面的RunnableScheduledFuture数组特别大，有898656个ScheduledFutureTask元素，占用了很多空间。

![](/assets/jvm/mat.png)

通过查看代码发现ScheduledFutureTask是在长连接建立时用来处理心跳续期的，在开启该任务时会存到一个map（用来存储长连接对应的任务，以便长连接断开时停止该任务）中，但在长连接断掉后关闭该任务时没从map中remove掉，所以这部分对象一直没有被回收，至此定位频繁Full GC的原因就是内存泄漏，增加remove的逻辑，问题解决。

#### 解决MQ广播模式导致的消息处理瓶颈问题

因为推送消息给客户端需要一个Channel对象，这个对象无法被序列化，只能与长连接ID形成一个映射关系存在内存中，在消费业务线mq发来的消息时无法知道消息里面对应的用户和哪些机器建立了长连接，所以刚开始是采用广播模式，每台机器都消费消息，然后去查出redis里面用户的长连接，再去映射关系里寻找Channel，发送消息。但是随着接入业务线的增加，消息量越来越多，高峰时每秒4，500的消息，因为是广播模式，实际相当于只有一台机器在消费，增加机器也没用，消息产生了积压，所以必须要改成集群模式。

为了解决开头提到的问题，新建了一张表存储服务器ID和consumerId的映射关系，首先根据机器数配置相应的consumerId到表中，每台服务器首次部署时会生成一个UUID作为服务器ID，且写到服务器一个目录下，以便下次启动时读取，接着去表中查询没有关联服务器ID的consumerId作为它的consumerId，并把关联关系更新到表中，并指定消费mq的tag为服务器ID。广播模式改成集群模式后，随机一台服务器消费消息，将消息入库，接着找到根据消息里用户去redis找到长连接，进而拿到长连接的服务器ID，将消息重新发到mq中并指定tag，只消费该tag的机器将负责推送消息给客户端。

#### 基于Lua脚本保证心跳续期的原子性

长连接服务的redis总共就两个key，一个存储长连接对应的用户ID、业务线和服务器ID，是个Hash结构，有效期是100秒，一个存储用户ID对应的长连接，是个zset结构，score存的是长连接过期时间的时间戳。建立长连接的时候会生成第一个key并向第二个key中添加元素，之后每隔20秒，前端会往后端发送一个心跳包，后端有一个字段记录心跳的最新事件，会更新该值，同时后端每隔45秒判断一下最近45秒内有没有收到心跳包，如果没有，则认为连接断开，清除redis相关的key并移除zset中对应的长连接，如果有，会延长hash结构的key的过期事件并更新zset中对应长连接的score值，这两步需要保证原子性，所以用了Lua。

#### 持续调优，将单机承载连接数从1000提升至100000；

- 应用

- JVM

  通过用mat分析dump和用gcviewer分析gc日志

- 数据库

  分库分表、慢sql持续优化

- 服务器

  调整了每台服务器的系统最大文件句柄数和单进程打开的最大句柄数，它们会限制长连接数

#### 基于Sentinel的限流与降级优先保证核心系统的消息可达性

上面提到的每天300万消息的业务线是核心业务线，且它只用长连接，所以为了保证流量过高时它不会出问题，我们对查询相关的一系列接口做了可限流的配置，流量高峰时提示用户系统繁忙，稍后重试。

#### 多数据源动态加载与扩展插件

- 配置文件里配置多个数据源的地址、用户名、密码

- 根据这些配置封装每个数据源对应的DruidDataSource对象，并作为value存放到一个map中，key为这些数据源的名称

- 把map设置给一个DynamicDataSource对象的targetDataSource属性，在原来返回DataSource的地方返回这个DynamicDataSource对象

- 维护一个ThreadLocal，存储每个线程对应的DataSource，在切换数据源时set，继承AbstractRoutingDataSource并重写它的determineCurrentLookupKey方法，在方法中从ThreadLocal里get出数据源，mybatis就会使用该数据源。

#### 线程池动态调参插件

- 继承ThreadPool封装一个带name的ThreadPoolExcutor，并存到一个以name为key，池为value的map中，创建线程池时通过该封装的类创建

- 在配置中心配置相应name的线程池配置

- 监听apollo，根据name获取到线程池，通过相关set方法调参，实时生效
