#### 线程安全三大问题

原子性、可见性、有序性，下面会从几个方面来分析这三个问题的原因及解决方案。

#### CPU缓存

- 高速缓存

  线程是CPU调度的最小单元，线程设计的目的最终仍然是更充分的利用计算机处理的效能，但是绝大部分的运算任务不能只依靠处理器就能完成，处理器还需要与内存交互，比如读取运算数据、存储运算结果，这个IO操作是很难消除的。而由于计算机的内存与处理器的运算速度差距非常大，所以现代计算机系统都会增加一层读写速度尽可能接近处理器运算速度的高速缓存来作为内存和处理器之间的缓冲：将运算需要使用的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步到内存之中。

  ![](/assets/concurrency/cpucache.png)

  从上到下依次为L3 cache、L2 cache、L1 cache，越接近CPU速度越快，同时容量也越小。

  - L1 Cache

    一级缓存，本地core的缓存，分成32K的数据缓存L1 d-cache和32k的指令缓存L1 i-cache，访问L1需要3cycles，耗时大约1ns；

  - L2 Cache

    二级缓存，本地core的缓存，被设计为L1缓存与共享的L3缓存之间的缓冲，大小为256K，访问L2需要12cycles，耗时大约3ns；

  - L3 Cache

    三级缓存，在同插槽的所有core共享L3缓存，分为多个2M的段，访问L3需要38cycles，耗时大约12ns；

- 缓存一致性问题

  比如有一个变量count，CPU-0读取主内存的数据，缓存到CPU-0的高速缓存中，CPU-1也做了同样的事情，同时CPU-1把count的值修改成了2，并且同步到CPU-1的高速缓存，但是这个修改之后的值并没有写入到主内存中，CPU-0访问该变量，由于缓存没有更新，所以仍然是之前的值，就会导致数据不一致的问题。引发这个问题的原因是因为多核心CPU情况下存在指令并行执行，而各个CPU核心之间的数据不共享从而导致缓存一致性问题，为了解决这个问题，CPU生产厂商提供了相应的解决方案。

  - 总线锁

    当一个处理器对其缓存中的数据进行操作的时候，往总线中发送一个Lock#信号，其他处理器对共享内存的请求将会被阻塞。其实在同一时刻，我们只需保证对某个内存地址的操作是原子性即可，但总线锁把处理器和内存之间的通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以这种方式会导致CPU的性能下降，P6系列以后的处理器，出现了另外一种方式，就是缓存锁。

  - 缓存锁

    内存区域如果被缓存在处理器的缓存行中并且在LOCK操作期间被锁定，那么当它执行锁操作回写到内存时，处理器不在总线上声明LOCK#信号，而是修改内部的内存地址，然后通过缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域的数据，当其他处理器回写已经被锁定的缓存行的数据时会导致当前处理器缓存行无效。每个处理器通过嗅探其他处理器访问内存和缓存来检查自己缓存的值是否已过期，当发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态。

  总线锁和缓存锁怎么选择，取决于很多因素，比如CPU是否支持、以及存在无法缓存的数据时 (比较大或者跨越多个缓存行的数据)，必然还是会使用总线锁。

- 缓存一致性协议

  处理器上有一套完整的协议，来保证缓存的一致性，比较经典的应该就是MESI协议了，它的方法是在CPU缓存中保存一个标记位，这个标记位有四种状态：

  - M(Modified)

    修改缓存，当前CPU缓存已经被修改，表示已经和内存中的数据不一致了。

  - E(Exclusive)

    独占缓存，当前CPU的缓存和内存中数据一致，而且其他处理器没有缓存该数据。

  - S(Shared)

    共享缓存，当前CPU的缓存和内存中数据一致，并且该数据存在于多个CPU的缓存中，每个处理器的缓存控制器不仅知道自己的读写操作，也监听其它处理器对缓存的读写操作。

  - I(Invalid)

    失效缓存，说明CPU的缓存已经不能使用了，得去内存重新读取。

#### 指令重排序

在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分3种类型：

- 编译器优化的重排序

  编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。

- 指令级并行重排序

  现代处理器采用了指令级并行技术（Instruction-Level Parallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。

- 内存系统的重排序

  处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。

从Java源代码到最终实际执行的指令序列，会分别经历下面3种重排序（2和3都属于处理器重排序）：

![](/assets/concurrency/reorder.png)

指令重排的目的是为了最大化的提高CPU利用率以及性能，CPU的乱序执行优化在单核时代并不影响正确性，但是在多核时代的多线程能够在不同的核心上实现真正的并行，一旦线程之间共享数据，就可能会出现一些不可预料的问题。指令重排序必须要遵循的原则是：不影响代码执行的最终结果，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序(这里所说的数据依赖性仅仅是针对单个处理器中执行的指令和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑) ，这个语义，实际上就是as-if-serial语义，不管怎么重排序，单线程程序的执行结果不会改变，编译器、处理器都必须遵守as-if-serial语义。

![](/assets/concurrency/reorderDemo.png)

如果不考虑编译器重排序和缓存可见性问题，上面这段代码可能会出现的结果是 x=0,y=1; x=1,y=0; x=1,y=1这三种结果，因为可能是先后执行t1/t2，也可能是反过来，还可能是t1/t2交替执行，但是这段代码的执行结果也有可能是x=0,y=0。这就是在乱序执行的情况下会导致的一种结果，因为线程t1内部的两行代码之间不存在数据依赖，因此可以把x=b乱序到a=1之前；同时线程t2中的y=a也可以早于t1中的a=1执行，那么他们的执行顺序可能是

t1: x=b

t2: b=1

t2: y=a

t1: a=1

所以从上面的例子来看，重排序会导致可见性问题。但是重排序带来的问题的严重性远远大于可见性，因为并不是所有指令都是简单的读或写，比如DCL的部分初始化问题。所以单纯的解决可见性问题还不够，还需要解决处理器重排序问题。
对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障（Memory Barriers，Intel称之为 Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。

#### 内存屏障

现在的CPU架构都提供了内存屏障功能：store barrier、load barrier、full barrier，主要的作用是:

- 防止指令之间的重排序

- 保证数据的可见性

在JMM中把内存屏障指令分为4类，通过在不同的语义下使用不同的内存屏障来禁止特定类型的处理器重排序，从而来保证内存的可见性：

- LoadLoad Barriers

  示例：load1 ; LoadLoad; load2 , 确保load1数据的装载先于load2及所有后续装载指令的装载

- StoreStore Barriers

  示例：store1; storestore;store2 , 确保store1数据对其他处理器可见（即刷新到内存）先于store2及所有后续存储指令的存储

- LoadStore Barries

  示例：load1;loadstore;store2, 确保load1数据装载先于store2以及后续的存储指令刷新到内存

- StoreLoad Barries

  示例：store1; storeload;load2, 确保store1数据对其他处理器变得可见（即刷新到内存）先于load2及所有后续装载指令的装载；这条内存屏障指令是一个全能型的屏障，会使该屏障之前的所有内存访问指令（不管是存储还是装载）完成之后，才执行该屏障之后的内存访问指令（不管是存储还是装载），也就是说它同时具有其他3条屏障的效果

总结：内存屏障只是解决顺序一致性问题，不解决缓存一致性问题，缓存一致性是由cpu的缓存锁以及MESI协议来完成的。而缓存一致性协议只关心缓存一致性，不关心顺序一致性。所以这是两个问题。

#### JMM

其实原子性、可见性、有序性问题，是我们抽象出来的概念，它们的核心本质就是刚刚提到的缓存一致性问题、处理器优化问题导致的指令重排序问题。 比如缓存一致性就导致可见性问题、处理器的乱序执行会导致原子性问题、指令重排会导致有序性问题。为了解决这些问题，所以在JVM中引入了JMM的概念。

JMM定义了共享内存系统中多线程程序读写操作行为的规范，来屏蔽各种硬件和操作系统的内存访问差异，来实现Java程序在各个平台下都能达到一致的内存访问效果。Java内存模型的主要目标是定义程序中各个变量的访问规则，也就是在JVM中将变量存储到内存以及从内存中取出变量（这里的变量，指的是共享变量，也就是实例对象、静态字段、数组对象等存储在堆内存中的变量。而对于局部变量、方法定义的参数（方法的传参）、异常处理器的参数（catch的异常）这类的，属于线程私有，不会被共享，即不会有可见性问题）这类的底层细节。通过这些规则来规范对内存的读写操作，从而保证指令执行的正确性。它解决了CPU多级缓存、处理器优化、指令重排等导致的内存访问问题，保证了并发场景下的可见性、原子性和有序性。

JMM决定一个线程对共享变量的写入何时对另一个线程可见，它定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程用来读写共享变量的副本。本地内存是JMM的抽象概念，并不真实存在，它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。线程对变量的所有操作（读取、赋值）都必须在工作内存中进行，不能直接读写主内存中的变量。并且不同的线程之间无法访问对方工作内存中的变量，线程间的变量值的传递都需要通过主内存来完成，它们三者的交互关系如下

![](/assets/concurrency/jmm1.png)

![](/assets/concurrency/jmm2.png)

JMM解决并发问题主要采用两种方式：限制处理器优化和使用内存屏障。但是JMM并没有主动限制处理器优化和指令重排序，也就是说在JMM这个模型之上，仍然会存在缓存一致性问题和指令重排序问题。JMM是一个抽象模型，它是建立在不同的操作系统和硬件层面之上 对问题进行了统一的抽象，然后在Java层面提供了一些高级指令如volatile、final等，让用户选择在合适的时候去引入这些高级指令来解决并发问题。

#### 如何解决原子性、可见性、有序性的问题

- 原子性

  在java中提供了两个高级的字节码指令monitorenter和monitorexit，在Java中对应的则是Synchronized来保证代码块内的操作是原子的。monitorenter和monitorexit隐式的执行了Lock和UnLock操作，Lock后其他线程无法获得锁，即使在执行过程中CPU时间片用完，线程也并没有进行解锁，而由于synchronized的锁是可重入的，下一个时间片还是只能被它自己获取到，还是会继续执行代码，直到所有代码执行完，这就保证了原子性。

- 可见性

  Java中的volatile关键字提供了一个功能，那就是被其修饰的变量在被修改后可以立即同步到主内存，被其修饰的变量在每次使用之前，JMM会把要使用该变量的线程的本地内存置为无效，线程接下来从主内存中读取。因此，可以使用volatile来保证多线程操作时变量的可见性。
  除了volatile，Java中的synchronized也可以实现可见性，线程A释放锁时，JMM会把A对应的本地内存的共享变量刷新到主内存中，而当线程B在申请同一个锁时，线程B的工作内存会被设置为无效，然后线程B会重新从主内存中加载它要访问的变量到它的工作内存中。
  final关键字也可以实现可见性。

- 有序性

  在Java中，可以使用synchronized和volatile来保证多线程之间操作的有序性。实现方式有所区别：volatile关键字会禁止指令重排，synchronized关键字保证同一时刻只允许一个线程操作，本质上就是as-if-serial语义保证了有序性。
